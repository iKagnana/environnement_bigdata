import os
import sys
import logging
from urllib.parse import urlparse
import glob
from pathlib import Path
import re

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def _parse_endpoint(endpoint: str):
    """
    Retourne (host:port, secure_bool) depuis une URL comme http://minio:9000 ou minio:9000
    """
    if not endpoint:
        return "minio:9000", False
    parsed = urlparse(endpoint if "://" in endpoint else f"http://{endpoint}")
    host = f"{parsed.hostname}:{parsed.port or 9000}"
    secure = parsed.scheme == "https"
    return host, secure

def create_spark_session():
    """Cr√©er une session Spark configur√©e pour MinIO (s3a)"""
    endpoint = os.environ.get("MINIO_ENDPOINT", "http://minio:9000")
    access_key = os.environ.get("MINIO_ACCESS_KEY", "minioadmin")
    secret_key = os.environ.get("MINIO_SECRET_KEY", "minioadmin123")

    host_port, secure = _parse_endpoint(endpoint)

    builder = SparkSession.builder \
        .appName("CSV to Parquet Converter - Healthcare Data") \
        .config("spark.hadoop.fs.s3a.endpoint", f"http://{host_port}" if not secure else f"https://{host_port}") \
        .config("spark.hadoop.fs.s3a.access.key", access_key) \
        .config("spark.hadoop.fs.s3a.secret.key", secret_key) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "true" if secure else "false") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")

    return builder.getOrCreate()

def create_minio_bucket(bucket_name="healthcare-data"):
    """Cr√©er le bucket MinIO si n√©cessaire"""
    try:
        from minio import Minio
    except Exception as e:
        logger.warning("minio SDK non install√© dans l'image : %s", e)
        return

    endpoint = os.environ.get("MINIO_ENDPOINT", "http://minio:9000")
    access_key = os.environ.get("MINIO_ACCESS_KEY", "minioadmin")
    secret_key = os.environ.get("MINIO_SECRET_KEY", "minioadmin123")

    host_port, secure = _parse_endpoint(endpoint)
    
    try:
        client = Minio(host_port, access_key=access_key, secret_key=secret_key, secure=secure)
        if not client.bucket_exists(bucket_name):
            logger.info("Cr√©ation du bucket MinIO: %s", bucket_name)
            client.make_bucket(bucket_name)
        else:
            logger.info("Bucket MinIO d√©j√† existant: %s", bucket_name)
    except Exception as e:
        logger.warning("Impossible de cr√©er ou v√©rifier le bucket '%s' : %s", bucket_name, e)

def clean_table_name(table_key):
    """
    Nettoie et valide un nom de table pour √©viter les erreurs PySpark
    
    Args:
        table_key (str): Nom de table brut
        
    Returns:
        str: Nom de table nettoy√© et valide
    """
    if not table_key:
        return "unknown_table"
    
    # Convertir en string au cas o√π
    table_key = str(table_key).lower()
    
    # Remplacer les espaces et caract√®res sp√©ciaux
    table_key = table_key.replace(" ", "_").replace("-", "_").replace(".", "_")
    
    # Enlever tous les caract√®res non alphanum√©riques sauf underscore
    table_key = re.sub(r'[^a-zA-Z0-9_]', '_', table_key)
    
    # Enlever les chiffres en d√©but de nom
    table_key = re.sub(r'^[^a-zA-Z]+', '', table_key)
    
    # Remplacer les underscores multiples par un seul
    table_key = re.sub(r'_+', '_', table_key)
    
    # Enlever les underscores en d√©but et fin
    table_key = table_key.strip('_')
    
    # S'assurer qu'on a un nom valide qui commence par une lettre
    if not table_key or not table_key[0].isalpha():
        table_key = f"table_{table_key}" if table_key else "unknown_table"
    
    return table_key

def discover_files_to_convert(base_path="/opt/app/datas", file_extension="csv"):
    """
    D√©couvre automatiquement tous les fichiers √† convertir dans un dossier
    
    Args:
        base_path (str): Chemin de base pour la recherche
        file_extension (str): Extension des fichiers √† chercher (csv, json, etc.)
    
    Returns:
        dict: Configuration pour chaque fichier trouv√©
    """
    files_config = {}
    
    # Pattern de recherche unique et r√©cursif
    search_pattern = f"{base_path}/**/*.{file_extension}"
    
    logger.info(f"üîç Recherche des fichiers .{file_extension} dans {base_path}...")
    
    # Utiliser un set pour √©viter les doublons
    files_found = set(glob.glob(search_pattern, recursive=True))
    
    logger.info(f"üìä {len(files_found)} fichiers uniques trouv√©s")
    
    for file_path in files_found:
        try:
            logger.info(f"üîç Traitement: {file_path}")
            
            path_obj = Path(file_path)
            
            # Cr√©er une cl√© unique bas√©e sur la structure des dossiers
            relative_path = path_obj.relative_to(base_path)
            parent_dirs = relative_path.parent.parts
            file_name = path_obj.stem.lower()
            
            logger.info(f"   üìÅ Parent dirs: {parent_dirs}")
            logger.info(f"   üìÑ File name: {file_name}")
            
            # Cr√©er un nom de table unique
            if parent_dirs:
                table_key = "_".join([str(d) for d in parent_dirs] + [file_name]).lower()
            else:
                table_key = file_name
            
            logger.info(f"   üîë Table key (avant nettoyage): {table_key}")
            
            # Nettoyer le nom de table AVANT toute utilisation
            table_key = clean_table_name(table_key)
            
            logger.info(f"   ‚ú® Table key (apr√®s nettoyage): {table_key}")
            
            # √âviter les doublons
            original_key = table_key
            counter = 1
            while table_key in files_config:
                table_key = f"{original_key}_{counter}"
                counter += 1
                logger.info(f"   ‚ö†Ô∏è Doublon d√©tect√©, nouveau nom: {table_key}")
            
            # IMPORTANT: S'assurer que table_key est bien une string
            table_key = str(table_key)
            
            # Calculer la taille du fichier de mani√®re s√©curis√©e
            try:
                file_size_bytes = os.path.getsize(file_path)
                # Utiliser le round natif de Python (pas celui de PySpark)
                file_size_mb = __builtins__.round(file_size_bytes / (1024 * 1024), 2)
            except (OSError, IOError) as size_error:
                logger.warning(f"‚ö†Ô∏è Impossible de calculer la taille de {file_path}: {size_error}")
                file_size_mb = 0.0
            
            # Construire les chemins en s'assurant que tout est string
            source_path_str = str(file_path)
            target_path_str = f"s3a://healthcare-data/silver/{table_key}/"
            
            files_config[table_key] = {
                "source_path": source_path_str,
                "target_path": target_path_str,
                "table_name": table_key,
                "original_path": source_path_str,
                "file_size_mb": file_size_mb
            }
            
            logger.info(f"‚úÖ Fichier enregistr√©: {table_key} ({file_size_mb} MB)")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du traitement de {file_path}: {str(e)}")
            import traceback
            logger.error(f"Traceback complet:\n{traceback.format_exc()}")
            continue
    
    if not files_config:
        logger.warning("‚ö†Ô∏è Aucun fichier trouv√©. Diagnostic:")
        for root, dirs, files in os.walk(base_path):
            matching_files = [f for f in files if f.endswith(f'.{file_extension}')]
            if matching_files:
                logger.warning(f"   üìÅ {root}: {matching_files}")
    
    logger.info(f"üì¶ Total de fichiers configur√©s: {len(files_config)}")
    return files_config

def convert_file_to_parquet(spark, source_path, target_path, table_name, file_format="csv"):
    """
    Convertit un fichier source en format Parquet et le sauvegarde dans MinIO
    
    Args:
        spark: SparkSession
        source_path (str): Chemin du fichier source
        target_path (str): Chemin cible dans MinIO
        table_name (str): Nom de la table
        file_format (str): Format du fichier source (csv, json, parquet, etc.)
    
    Returns:
        bool: True si conversion r√©ussie, False sinon
    """
    try:
        # Convertir tous les param√®tres en string de mani√®re d√©fensive
        source_path = str(source_path) if source_path is not None else ""
        target_path = str(target_path) if target_path is not None else ""
        table_name = str(table_name) if table_name is not None else "unknown_table"
        file_format = str(file_format) if file_format is not None else "csv"
        
        logger.info(f"üîÑ D√©but de la conversion pour {table_name}")
        logger.info(f"   Source: {source_path}")
        logger.info(f"   Cible: {target_path}")
        
        # V√©rifier l'existence du fichier source
        if not os.path.exists(source_path):
            logger.error(f"‚ùå Fichier source non trouv√©: {source_path}")
            return False
        
        # Configuration de lecture selon le format
        if file_format.lower() == "csv":
            try:
                df = spark.read \
                    .option("header", "true") \
                    .option("inferSchema", "true") \
                    .option("multiline", "true") \
                    .option("escape", '"') \
                    .option("quote", '"') \
                    .option("ignoreLeadingWhiteSpace", "true") \
                    .option("ignoreTrailingWhiteSpace", "true") \
                    .csv(source_path)
            except Exception as read_error:
                logger.warning(f"‚ö†Ô∏è Erreur avec inferSchema, tentative sans: {str(read_error)[:100]}...")
                # Fallback sans inf√©rence de sch√©ma
                df = spark.read \
                    .option("header", "true") \
                    .option("multiline", "true") \
                    .option("escape", '"') \
                    .option("quote", '"') \
                    .csv(source_path)
        
        elif file_format.lower() == "json":
            df = spark.read \
                .option("multiline", "true") \
                .json(source_path)
        
        elif file_format.lower() == "parquet":
            df = spark.read.parquet(source_path)
        
        else:
            logger.error(f"‚ùå Format non support√©: {file_format}")
            return False
        
        # Ajout de m√©tadonn√©es de tra√ßabilit√© avec gestion d'erreurs
        try:
            df_with_metadata = df \
                .withColumn("conversion_timestamp", current_timestamp()) \
                .withColumn("source_file", lit(source_path)) \
                .withColumn("table_name", lit(table_name))
        except Exception as metadata_error:
            logger.warning(f"‚ö†Ô∏è Erreur lors de l'ajout des m√©tadonn√©es pour {table_name}: {metadata_error}")
            # Fallback sans m√©tadonn√©es
            df_with_metadata = df
        
        # V√©rifier que le DataFrame n'est pas vide
        try:
            row_count = df_with_metadata.count()
            if row_count == 0:
                logger.warning(f"‚ö†Ô∏è Le fichier {table_name} est vide")
                return False
        except Exception as count_error:
            logger.warning(f"‚ö†Ô∏è Impossible de compter les lignes de {table_name}: {count_error}")
            row_count = -1
        
        # Optimisation : repartitionnement si n√©cessaire
        if row_count > 0:
            if row_count > 1000000:  # Plus d'1M de lignes
                df_with_metadata = df_with_metadata.repartition(8)
            elif row_count > 100000:  # Plus de 100K lignes
                df_with_metadata = df_with_metadata.repartition(4)
            else:
                df_with_metadata = df_with_metadata.coalesce(1)
        
        # Sauvegarde en Parquet dans MinIO
        logger.info(f"üíæ Sauvegarde en cours vers MinIO...")
        df_with_metadata.write \
            .mode("overwrite") \
            .option("compression", "snappy") \
            .parquet(target_path)
        
        # Statistiques
        column_count = len(df_with_metadata.columns)
        logger.info(f"‚úÖ Conversion r√©ussie pour {table_name}")
        logger.info(f"   üìä {row_count:,} lignes, {column_count} colonnes")
        logger.info(f"   üìç Localisation: {target_path}")
        
        # Aper√ßu des donn√©es (limit√© pour √©viter les erreurs d'affichage)
        logger.info("üëÄ Aper√ßu des donn√©es:")
        try:
            df_with_metadata.show(3, truncate=True)
        except Exception as show_error:
            logger.warning(f"‚ö†Ô∏è Impossible d'afficher l'aper√ßu: {str(show_error)[:50]}...")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la conversion de {str(table_name)}: {str(e)}")
        import traceback
        logger.debug(f"Traceback complet: {traceback.format_exc()}")
        return False

def convert_folder_to_parquet(source_folder="/opt/app/datas", file_extension="csv", 
                            target_bucket="healthcare-data"):
    """
    Fonction principale pour convertir tous les fichiers d'un dossier en Parquet
    
    Args:
        source_folder (str): Dossier source contenant les fichiers
        file_extension (str): Extension des fichiers √† convertir
        target_bucket (str): Nom du bucket MinIO cible
    
    Returns:
        dict: Statistiques de conversion
    """
    
    # Cr√©er le bucket MinIO
    create_minio_bucket(target_bucket)
    
    # Cr√©er la session Spark
    spark = create_spark_session()
    
    stats = {
        "total_files": 0,
        "successful_conversions": 0,
        "failed_conversions": 0,
        "files_processed": []
    }
    
    try:
        # D√©couvrir tous les fichiers √† convertir
        files_config = discover_files_to_convert(source_folder, file_extension)
        stats["total_files"] = len(files_config)
        
        if not files_config:
            logger.error("‚ùå Aucun fichier √† convertir trouv√©")
            return stats
        
        logger.info(f"üöÄ D√©but de la conversion de {len(files_config)} fichiers {file_extension.upper()} vers Parquet")
        
        # Convertir chaque fichier
        for table_name, config in files_config.items():
            logger.info(f"üìÑ Traitement de {table_name} ({stats['successful_conversions'] + stats['failed_conversions'] + 1}/{stats['total_files']})")
            
            success = convert_file_to_parquet(
                spark=spark,
                source_path=config["source_path"],
                target_path=config["target_path"],
                table_name=table_name,
                file_format=file_extension
            )
            
            if success:
                stats["successful_conversions"] += 1
                stats["files_processed"].append({
                    "table": table_name,
                    "status": "SUCCESS",
                    "source": config["source_path"],
                    "target": config["target_path"],
                    "size_mb": config["file_size_mb"]
                })
            else:
                stats["failed_conversions"] += 1
                stats["files_processed"].append({
                    "table": table_name,
                    "status": "FAILED",
                    "source": config["source_path"],
                    "error": "Conversion failed"
                })
        
        # R√©sum√© final
        logger.info("=" * 50)
        logger.info("üéâ R√âSUM√â DE LA CONVERSION")
        logger.info("=" * 50)
        logger.info(f"üìä Total de fichiers: {stats['total_files']}")
        logger.info(f"‚úÖ Conversions r√©ussies: {stats['successful_conversions']}")
        logger.info(f"‚ùå Conversions √©chou√©es: {stats['failed_conversions']}")
        
        if stats['total_files'] > 0:
            success_rate = (stats['successful_conversions']/stats['total_files']*100)
            logger.info(f"üìà Taux de r√©ussite: {success_rate:.1f}%")
        
        # D√©tail des fichiers trait√©s
        for file_info in stats["files_processed"]:
            status_icon = "‚úÖ" if file_info["status"] == "SUCCESS" else "‚ùå"
            size_info = f"{file_info.get('size_mb', 'N/A')} MB" if file_info["status"] == "SUCCESS" else "Failed"
            logger.info(f"{status_icon} {file_info['table']}: {size_info}")
        
        return stats
        
    except Exception as e:
        logger.error(f"üí• Erreur g√©n√©rale: {str(e)}")
        stats["failed_conversions"] = stats["total_files"]
        return stats
    finally:
        if spark:
            spark.stop()

def main():
    """Point d'entr√©e principal"""
    logger.info("üöÄ D√©marrage du convertisseur CSV vers Parquet")
    
    # Configuration (peut √™tre modifi√©e selon vos besoins)
    source_folder = os.environ.get("SOURCE_FOLDER", "/opt/app/datas")
    file_extension = os.environ.get("FILE_EXTENSION", "csv")
    target_bucket = os.environ.get("TARGET_BUCKET", "healthcare-data")
    
    logger.info(f"üìÅ Dossier source: {source_folder}")
    logger.info(f"üìÑ Extension recherch√©e: {file_extension}")
    logger.info(f"ü™£ Bucket cible: {target_bucket}")
    
    # Lancer la conversion
    stats = convert_folder_to_parquet(
        source_folder=source_folder,
        file_extension=file_extension,
        target_bucket=target_bucket
    )
    
    # Sortir avec le code appropri√©
    if stats["failed_conversions"] == 0 and stats["successful_conversions"] > 0:
        logger.info("üéâ Toutes les conversions ont r√©ussi!")
        sys.exit(0)
    elif stats["total_files"] == 0:
        logger.warning("‚ö†Ô∏è Aucun fichier trouv√© √† convertir")
        sys.exit(0)
    else:
        logger.error(f"‚ö†Ô∏è {stats['failed_conversions']} conversions ont √©chou√© sur {stats['total_files']}")
        sys.exit(1)

if __name__ == "__main__":
    main()
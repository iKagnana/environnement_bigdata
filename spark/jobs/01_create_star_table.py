import os
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def _parse_endpoint(endpoint: str):
    """
    Retourne (host:port, secure_bool) depuis une URL comme http://minio:9000 ou minio:9000
    """
    from urllib.parse import urlparse
    if not endpoint:
        return "minio:9000", False
    parsed = urlparse(endpoint if "://" in endpoint else f"http://{endpoint}")
    host = f"{parsed.hostname}:{parsed.port or 9000}"
    secure = parsed.scheme == "https"
    return host, secure

def create_spark_session():
    """CrÃ©er une session Spark configurÃ©e pour MinIO (s3a) et Delta Lake"""
    endpoint = os.environ.get("MINIO_ENDPOINT", "http://minio:9000")
    access_key = os.environ.get("MINIO_ACCESS_KEY", "minioadmin")
    secret_key = os.environ.get("MINIO_SECRET_KEY", "minioadmin123")

    host_port, secure = _parse_endpoint(endpoint)

    builder = SparkSession.builder \
        .appName("Create Delta Tables - Healthcare Data") \
        .config("spark.hadoop.fs.s3a.endpoint", f"http://{host_port}" if not secure else f"https://{host_port}") \
        .config("spark.hadoop.fs.s3a.access.key", access_key) \
        .config("spark.hadoop.fs.s3a.secret.key", secret_key) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "true" if secure else "false") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")

    return builder.getOrCreate()

def create_delta_table_safe(spark, table_name, schema, partition_by, path):
    """CrÃ©er une table Delta de maniÃ¨re sÃ©curisÃ©e avec gestion d'erreur"""
    try:
        logger.info(f"CrÃ©ation de la table {table_name}...")

        # CrÃ©er un DataFrame vide avec le schÃ©ma - utiliser sparkContext directement
        # pour Ã©viter les problÃ¨mes de sÃ©rialisation
        empty_rdd = spark.sparkContext.emptyRDD()
        df = spark.createDataFrame(empty_rdd, schema)

        writer = df.write.format("delta").mode("ignore").option("overwriteSchema", "true")

        if partition_by:
            if isinstance(partition_by, list):
                writer = writer.partitionBy(*partition_by)
            else:
                writer = writer.partitionBy(partition_by)

        writer.save(path)
        logger.info(f"âœ… Table {table_name} crÃ©Ã©e avec succÃ¨s")
        return True
    except Exception as e:
        logger.error(f"âŒ Erreur lors de la crÃ©ation de {table_name}: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def create_delta_tables(spark):
    """CrÃ©er toutes les tables Delta selon le schÃ©ma dÃ©fini"""
    logger.info("ðŸš€ CrÃ©ation des tables Delta Healthcare...")

    tables_config = []

    # =================================
    # TABLES DE DIMENSIONS
    # =================================

    # Dimension des lieux
    tables_config.append({
        "name": "dim_lieu",
        "schema": StructType([
            StructField("lieu_id", IntegerType(), True),
            StructField("commune", StringType(), True),
            StructField("departement", StringType(), True),
            StructField("region", StringType(), True),
            StructField("pays", StringType(), True)
        ]),
        "partition_by": "region",
        "path": "s3a://healthcare-data/gold/dim_lieu"
    })

    # Dimension des patients
    tables_config.append({
        "name": "dim_patient",
        "schema": StructType([
            StructField("patient_id", IntegerType(), True),
            StructField("age", IntegerType(), True),
            StructField("sexe", StringType(), True),
            StructField("annee_naissance", IntegerType(), True),
            StructField("date_naissance", DateType(), True)
        ]),
        "partition_by": "annee_naissance",
        "path": "s3a://healthcare-data/gold/dim_patient"
    })

    # Dimension des dates
    tables_config.append({
        "name": "dim_date",
        "schema": StructType([
            StructField("date_id", IntegerType(), True),
            StructField("date_complete", DateType(), True),
            StructField("annee", IntegerType(), True),
            StructField("mois", IntegerType(), True),
            StructField("jour", IntegerType(), True),
            StructField("trimestre", IntegerType(), True),
            StructField("semestre", IntegerType(), True)
        ]),
        "partition_by": "annee",
        "path": "s3a://healthcare-data/gold/dim_date"
    })

    # Dimension des Ã©tablissements
    tables_config.append({
        "name": "dim_etablissement",
        "schema": StructType([
            StructField("etablissement_id", IntegerType(), True),
            StructField("raison_sociale", StringType(), True),
            StructField("adresse", StringType(), True),
            StructField("commune", StringType(), True),
            StructField("code_postal", StringType(), True),
            StructField("region", StringType(), True)
        ]),
        "partition_by": "region",
        "path": "s3a://healthcare-data/gold/dim_etablissement"
    })

    # Dimension des diagnostics
    tables_config.append({
        "name": "dim_diagnostic",
        "schema": StructType([
            StructField("diagnostic_id", StringType(), True),
            StructField("libelle_diagnostic", StringType(), True),
            StructField("categorie_diagnostic", StringType(), True)
        ]),
        "partition_by": "categorie_diagnostic",
        "path": "s3a://healthcare-data/gold/dim_diagnostic"
    })

    # Dimension des indicateurs
    tables_config.append({
        "name": "dim_indicateur",
        "schema": StructType([
            StructField("indicateur_id", IntegerType(), True),
            StructField("libelle_indicateur", StringType(), True),
            StructField("categorie_indicateur", StringType(), True)
        ]),
        "partition_by": "categorie_indicateur",
        "path": "s3a://healthcare-data/gold/dim_indicateur"
    })

    # Dimension des professionnels
    tables_config.append({
        "name": "dim_professionel",
        "schema": StructType([
            StructField("professionnel_id", IntegerType(), True),
            StructField("nom", StringType(), True),
            StructField("prenom", StringType(), True),
            StructField("civilite", StringType(), True),
            StructField("profession", StringType(), True),
            StructField("specialite", StringType(), True)
        ]),
        "partition_by": ["profession", "specialite"],
        "path": "s3a://healthcare-data/gold/dim_professionel"
    })

    # =================================
    # TABLES DE FAITS
    # =================================

    # Fait dÃ©cÃ¨s
    tables_config.append({
        "name": "fait_deces",
        "schema": StructType([
            StructField("fk_patient", IntegerType(), True),
            StructField("fk_date_deces", IntegerType(), True),
            StructField("fk_lieu_deces", IntegerType(), True),
            StructField("fk_lieu_naissance", IntegerType(), True),
            StructField("nb_deces", IntegerType(), True),
            StructField("annee", IntegerType(), True)
        ]),
        "partition_by": "annee",
        "path": "s3a://healthcare-data/gold/fait_deces"
    })

    # Fait hospitalisation
    tables_config.append({
        "name": "fait_hospitalisation",
        "schema": StructType([
            StructField("fk_patient", IntegerType(), True),
            StructField("fk_etablissement", IntegerType(), True),
            StructField("fk_diagnostic", StringType(), True),
            StructField("fk_date_entree", IntegerType(), True),
            StructField("duree_sejour_jours", IntegerType(), True),
            StructField("nb_hospitalisations", IntegerType(), True),
            StructField("annee", IntegerType(), True)
        ]),
        "partition_by": "annee",
        "path": "s3a://healthcare-data/gold/fait_hospitalisation"
    })

    # Fait satisfaction
    tables_config.append({
        "name": "fait_satisfaction",
        "schema": StructType([
            StructField("fk_etablissement", IntegerType(), True),
            StructField("fk_indicateur", IntegerType(), True),
            StructField("fk_date_mesure", IntegerType(), True),
            StructField("score_ajuste", FloatType(), True),
            StructField("nombre_reponses", IntegerType(), True),
            StructField("taux_participation", FloatType(), True),
            StructField("annee", IntegerType(), True)
        ]),
        "partition_by": "annee",
        "path": "s3a://healthcare-data/gold/fait_satisfaction"
    })

    # Fait consultations
    tables_config.append({
        "name": "fait_consultations",
        "schema": StructType([
            StructField("fk_patient", IntegerType(), True),
            StructField("fk_professionnel", IntegerType(), True),
            StructField("fk_date_consultation", IntegerType(), True),
            StructField("fk_diagnostic", StringType(), True),
            StructField("fk_etablissement", IntegerType(), True),
            StructField("duree_consultation_minutes", IntegerType(), True),
            StructField("nb_consultations", IntegerType(), True),
            StructField("annee", IntegerType(), True)
        ]),
        "partition_by": "annee",
        "path": "s3a://healthcare-data/gold/fait_consultations"
    })

    # CrÃ©er toutes les tables
    success_count = 0
    failed_count = 0

    for table_config in tables_config:
        if create_delta_table_safe(
            spark,
            table_config["name"],
            table_config["schema"],
            table_config["partition_by"],
            table_config["path"]
        ):
            success_count += 1
        else:
            failed_count += 1

    logger.info("=" * 60)
    logger.info(f"âœ… Tables crÃ©Ã©es avec succÃ¨s: {success_count}")
    logger.info(f"âŒ Tables Ã©chouÃ©es: {failed_count}")
    logger.info(f"ðŸ“Š Total: {success_count + failed_count}")
    logger.info("=" * 60)

    return success_count, failed_count

def verify_tables(spark):
    """VÃ©rifier que toutes les tables ont Ã©tÃ© crÃ©Ã©es"""
    logger.info("ðŸ” VÃ©rification des tables crÃ©Ã©es...")

    tables = [
        ("dim_lieu", "s3a://healthcare-data/gold/dim_lieu"),
        ("dim_patient", "s3a://healthcare-data/gold/dim_patient"),
        ("dim_date", "s3a://healthcare-data/gold/dim_date"),
        ("dim_etablissement", "s3a://healthcare-data/gold/dim_etablissement"),
        ("dim_diagnostic", "s3a://healthcare-data/gold/dim_diagnostic"),
        ("dim_indicateur", "s3a://healthcare-data/gold/dim_indicateur"),
        ("dim_professionel", "s3a://healthcare-data/gold/dim_professionel"),
        ("fait_deces", "s3a://healthcare-data/gold/fait_deces"),
        ("fait_hospitalisation", "s3a://healthcare-data/gold/fait_hospitalisation"),
        ("fait_satisfaction", "s3a://healthcare-data/gold/fait_satisfaction"),
        ("fait_consultations", "s3a://healthcare-data/gold/fait_consultations")
    ]

    verified_count = 0
    for table_name, table_path in tables:
        try:
            df = spark.read.format("delta").load(table_path)
            col_count = len(df.columns)
            row_count = df.count()
            logger.info(f"âœ… Table {table_name}: {col_count} colonnes, {row_count} lignes")
            verified_count += 1
        except Exception as e:
            logger.error(f"âŒ Erreur pour la table {table_name}: {str(e)}")

    logger.info(f"ðŸ“Š Tables vÃ©rifiÃ©es: {verified_count}/{len(tables)}")

def main():
    """Fonction principale pour crÃ©er les tables Delta"""
    spark = None

    try:
        logger.info("ðŸš€ DÃ©but de la crÃ©ation des tables Delta Healthcare...")

        # CrÃ©er la session Spark
        spark = create_spark_session()

        # CrÃ©er toutes les tables
        success_count, failed_count = create_delta_tables(spark)

        # VÃ©rifier les tables crÃ©Ã©es seulement si tout s'est bien passÃ©
        if failed_count == 0:
            verify_tables(spark)

        logger.info("ðŸŽ‰ CrÃ©ation des tables Delta terminÃ©e!")

        # Retourner un code de sortie appropriÃ©
        if failed_count > 0:
            import sys
            sys.exit(1)

    except Exception as e:
        logger.error(f"ðŸ’¥ Erreur lors de la crÃ©ation des tables: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        import sys
        sys.exit(1)
    finally:
        if spark is not None:
            try:
                spark.stop()
            except:
                pass

if __name__ == "__main__":
    main()

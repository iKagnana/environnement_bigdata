{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901a881c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\alexandrecougny\\onedrive\\bureau\\cesi\\a4\\bloc big data\\projet chu\\projet_big_data\\.venv\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\alexandrecougny\\onedrive\\bureau\\cesi\\a4\\bloc big data\\projet chu\\projet_big_data\\.venv\\lib\\site-packages (from pyspark) (0.10.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9bf784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Initialisation de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Evaluation Performance Delta Lake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Table de résultats\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS delta.`s3a://chu/delta/evaluation_performance` (\n",
    "    requete STRING,\n",
    "    configuration STRING,\n",
    "    temps_execution_s DOUBLE\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Fonction de mesure de performance\n",
    "def mesurer_temps_requete(nom_requete, configuration, requete_sql, nb_repetitions=10):\n",
    "    temps_exec = []\n",
    "    for i in range(nb_repetitions):\n",
    "        t0 = time.time()\n",
    "        spark.sql(requete_sql).collect()  # Exécution complète\n",
    "        t1 = time.time()\n",
    "        temps_exec.append(t1 - t0)\n",
    "    \n",
    "    temps_moyen = sum(temps_exec) / len(temps_exec)\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO delta.`s3a://chu/delta/evaluation_performance`\n",
    "        VALUES ('{nom_requete}', '{configuration}', {temps_moyen})\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"{nom_requete} ({configuration}) : {temps_moyen:.3f} s (moyenne sur {nb_repetitions} exécutions)\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Exemple 1 : requête sur table non partitionnée\n",
    "# -------------------------------------------------------------------\n",
    "requete_non_part = \"\"\"\n",
    "SELECT fk_etablissement,\n",
    "       COUNT(DISTINCT fk_patient) * 100.0 / COUNT(fk_patient) AS taux_consultation\n",
    "FROM delta.`s3a://chu/delta/fait_consultations`\n",
    "WHERE fk_etablissement = 'ETAB_X'\n",
    "    AND date_consult BETWEEN '2020-01-01' AND '2020-12-31'\n",
    "GROUP BY fk_etablissement\n",
    "\"\"\"\n",
    "\n",
    "mesurer_temps_requete(\n",
    "    nom_requete=\"Taux de consultation par établissement en 2020\",\n",
    "    configuration=\"Non partitionnée\",\n",
    "    requete_sql=requete_non_part\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Exemple 2 : requête sur table partitionnée\n",
    "# -------------------------------------------------------------------\n",
    "requete_part = \"\"\"\n",
    "SELECT fk_etablissement,\n",
    "    COUNT(DISTINCT fk_patient) * 100.0 / COUNT(fk_patient) AS taux_consultation\n",
    "FROM delta.`s3a://chu/delta/fait_consultations_partitionnee`\n",
    "WHERE fk_etablissement = 'ETAB_X'\n",
    "    AND date_consult BETWEEN '2020-01-01' AND '2020-12-31'\n",
    "GROUP BY fk_etablissement\n",
    "\"\"\"\n",
    "\n",
    "mesurer_temps_requete(\n",
    "    nom_requete=\"Taux de consultation par établissement en 2020\",\n",
    "    configuration=\"Partitionnée\",\n",
    "    requete_sql=requete_part\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Lecture des résultats et visualisation\n",
    "# -------------------------------------------------------------------\n",
    "# Conversion en DataFrame Pandas pour affichage\n",
    "df_perf = spark.read.format(\"delta\").load(\"s3a://chu/delta/evaluation_performance\").toPandas()\n",
    "\n",
    "# Graphique de comparaison\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(df_perf[\"configuration\"], df_perf[\"temps_execution_s\"], color=['darkred', 'steelblue'])\n",
    "plt.title(\"Comparaison des temps d'exécution (partitionnée vs non partitionnée)\")\n",
    "plt.xlabel(\"Configuration\")\n",
    "plt.ylabel(\"Temps d'exécution moyen (s)\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
